# Credit Card Transaction Fraud Detection 
The project began with a data quality report where all data from the 18 fields were analyzed. There are two numerical fields - date and amount - while the rest of the fields were categorical. A distribution is provided for each relevant field with a histogram or line plot, which can be found in the Appendix. Most importantly, the dataset had 1,059 occurrences of fraud out of 96,753 records.

Before further investigation, the data had to be properly cleaned. Eight extraneous empty fields were dropped and one outlier was removed due to its extremely high value. Transactions that were not purchases, ‘P’, were filtered out. The following fields: Merchnum, Merch state, and Merch zip, contain missing values. Missing values were filled with carefully designed data imputation techniques.

With the data cleaned, as many as possible candidate variables were created. These variables try to capture signals of fraud that include bursts of activities at different merchants, cards used at merchants never used before, larger than normal purchase amounts, etc. 1,191 variables were created that include amount, frequency, day-since, velocity change, Benford’s law, etc. Benford’s law variables were removed because while they are viable for forensic analysis, they were not properly formed for this project.

After a total of 1,191 variables were created, feature selection was performed through forward selection and a catboost wrapper to select the best 20 variables to deploy in machine learning models. Kolmogorov-Smirnov (KS) and Fraud Detection Rate (FDR) at 3% were used to filter out ineffective variables. Finally, the data is divided into three sections for model analysis: training, testing, and out-of-time (OOT). For Training and testing datasets, the model will use the data before November 1st 2010, and OOT is scored on data after that date.

To determine the best model, multiple models were tested with different hyperparameters. Logistic regression is the base linear model. Nonlinear models like random forest, boosted trees, and neural networks were tested after. Light gradient-boosted machine (LGBM) was the best model due to similar training and testing performance, and good OOT score. With light gradient-boosted machine, 53.79% of fraud can be eliminated by declining 3% of total applications to save around $21,480,000 per year.

In short, the results for this credit card transaction fraud analysis are as follows. Concerning the training dataset, the model can eliminate 82.43% of fraud by declining 3% of the total applications. The model can eliminate 80.31% of fraud by declining 3% of the total applications with the testing dataset. The model can eliminate 53.79% of fraud by declining 3% of the total applications with the OOT dataset.
